{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from stqdm import stqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speaker_embedding import preprocess_wav, VoiceEncoder\n",
    "from demo_utils import *\n",
    "from itertools import groupby\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import argparse, sys, os, json, codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "cmaps = plt.colormaps()\n",
    "plt.rcParams[\"font.family\"] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from umap import UMAP\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Segment Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(path_sample):\n",
    "    wav_fpaths = list(Path(path_sample).glob(\"*.wav\"))\n",
    "    voice_type = list(map(lambda wav_fpath: wav_fpath.parent.stem, wav_fpaths))\n",
    "    wavs = np.array(list(map(preprocess_wav, stqdm(\n",
    "        wav_fpaths, \"Preprocessing wavs\", len(wav_fpaths), position=0))))\n",
    "    st.success('Finished preprocessing')\n",
    "\n",
    "    encoder = VoiceEncoder()\n",
    "    utterance_embeds = np.array(\n",
    "        list(map(encoder.embed_utterance, stqdm(wavs, \"compute emb vec.\", len(wavs)))))\n",
    "    st.success('Finished embedding compute')\n",
    "\n",
    "    voice_type_wavs = {speaker: wavs[list(indices)] for speaker, indices in\n",
    "                        stqdm(groupby(range(len(wavs)), lambda i: voice_type[i]))}\n",
    "    voice_type_embeds = np.array([encoder.embed_speaker(wavs[:len(wavs)])\n",
    "                                    for wavs in stqdm(voice_type_wavs.values())])\n",
    "    st.success('Finished voice-type embeddings')\n",
    "\n",
    "    return voice_type, wavs, utterance_embeds, voice_type_wavs, voice_type_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_path = \"/home/kwangje/Desktop/sr-iptv-proto/embeds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeds_path+'/wavs/wavs_child123_v2_1.pkl','rb') as f:\n",
    "    wavs_child_123_real = pickle.load(f)\n",
    "\n",
    "with open(embeds_path+'/wavs/wavs_adult_female_v2_1.pkl','rb') as f:\n",
    "    wavs_adult_female_real = pickle.load(f)\n",
    "\n",
    "with open(embeds_path+'/wavs/wavs_adult_male_v2_1.pkl','rb') as f:\n",
    "    wavs_adult_male_real = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeds_path+'/real/voice_type_embeds_child123_v2_1.pkl','rb') as f:\n",
    "    embeds_child_123_real = pickle.load(f)\n",
    "\n",
    "with open(embeds_path+'/real/voice_type_embeds_adult_male_v2_1.pkl','rb') as f:\n",
    "    embeds_adult_female_real = pickle.load(f)\n",
    "\n",
    "with open(embeds_path+'/real/voice_type_embeds_adult_female_v2_1.pkl','rb') as f:\n",
    "    embeds_adult_male_real = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cpu in 0.01 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module.load_state_dict of VoiceEncoder(\n",
       "  (lstm): LSTM(40, 256, num_layers=3, batch_first=True)\n",
       "  (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       ")>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from speaker_embedding import preprocess_wav, VoiceEncoder\n",
    "\n",
    "encoder = VoiceEncoder()\n",
    "encoder.load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeds_path+'/utter/utterance_embeds_child123.pkl','rb') as f:\n",
    "    utterance_embeds_child123 = pickle.load(f)\n",
    "\n",
    "with open(embeds_path+'/utter/utterance_embeds_female.pkl','rb') as f:\n",
    "    utterance_embeds_female = pickle.load(f)\n",
    "\n",
    "with open(embeds_path+'/utter/utterance_embeds_male.pkl','rb') as f:\n",
    "    utterance_embeds_male = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization for age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist(speaker_wavs, spk_embeds_total, test_embed):\n",
    "    spk_sim_matrix = np.inner(spk_embeds_total, test_embed)\n",
    "\n",
    "    labels = [i for i in speaker_wavs.keys()]\n",
    "    stats = dict(zip(labels, spk_sim_matrix))\n",
    "\n",
    "    fig = go.Figure([go.Bar(x=labels, y=spk_sim_matrix)])\n",
    "\n",
    "    result = max(stats, key=stats.get)\n",
    "\n",
    "    fig.update_layout(margin=dict(l=5, r=5, b=5, t=5))\n",
    "    st.plotly_chart(fig)\n",
    "    return stats, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "utterance_embeds_female     61362\n",
    "utterance_embeds_male       59214\n",
    "utterance_embeds_child123   50849\n",
    "total                       171425\n",
    "'''\n",
    "\n",
    "utterance_embeds_total = np.append(utterance_embeds_female, utterance_embeds_male, axis=0)\n",
    "utterance_embeds_total = np.append(utterance_embeds_total, utterance_embeds_child123, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = UMAP(n_neighbors=30, min_dist=0.2, n_components=3)\n",
    "proj_3d = reducer.fit_transform(utterance_embeds_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(proj_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:61361, \"voice_type\"] = 'female'\n",
    "df.loc[61362:120576, \"voice_type\"] = \"male\"\n",
    "df.loc[120576:, \"voice_type\"] = \"child\"\n",
    "#df.loc[:61361, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female    61362\n",
       "male      59214\n",
       "child     50849\n",
       "Name: voice_type, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"voice_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_3d = px.scatter_3d(\n",
    "    proj_3d, x=0, y=1, z=2,\n",
    "    color=df.voice_type, labels={'color': 'voice_type'}, opacity=0.7)\n",
    "    \n",
    "fig_3d.update_traces(marker_size=1)\n",
    "fig_3d.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig_3d.write_html(\"clf_dvector.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### caculate the similarity between the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_similarity(self, embeds_child, embeds_adult_female, embeds_adult_male, test):\n",
    "    labels = [\"child\", \"adult_female\", \"adult_male\", \"unknown\"]\n",
    "\n",
    "    sim_child = np.inner(embeds_child, test)\n",
    "    sim_adult_fm = np.inner(embeds_adult_female, test)\n",
    "    sim_adult_m = np.inner(embeds_adult_male, test)\n",
    "\n",
    "    if (sim_child and sim_adult_fm and sim_adult_m) < 0.5:\n",
    "        sim_unk = int(1.0)\n",
    "    else:\n",
    "        sim_unk = int(0)\n",
    "\n",
    "    similarities = [sim_child, sim_adult_fm, sim_adult_m, sim_unk]\n",
    "\n",
    "    stats = dict(zip(labels, similarities))\n",
    "    result = max(stats, key=stats.get)\n",
    "\n",
    "    return stats, result, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_clf_single(self, path_audio_file):\n",
    "    \"\"\"\n",
    "    :param audio_file (str): Path to audio file to predict\n",
    "    :return answer (str): voice-type predicted by the model\n",
    "    \"\"\"\n",
    "\n",
    "    test_path = Path(path_audio_file)\n",
    "    test_wav = preprocess_wav(test_path)\n",
    "    test_embed = self.encoder.embed_utterance(test_wav)\n",
    "    score, answer, sim = self.cal_similarity(\n",
    "        self.embeds_child_123,\n",
    "        self.embeds_adult_female,\n",
    "        self.embeds_adult_male,\n",
    "        test_embed,\n",
    "    )\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = args.dst\n",
    "    output_folder = args.output\n",
    "    test_result = age_clf(audio_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavencoder",
   "language": "python",
   "name": "wavencoder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}